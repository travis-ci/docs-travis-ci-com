---
title: S3 Deployment
layout: en
permalink: /user/deployment/s3/
---

Travis CI can automatically upload your build to S3 after a successful build.

For a minimal configuration, all you need to do is add the following to your `.travis.yml`:

{% highlight yaml %}
deploy:
  provider: s3
  access_key_id: "YOUR AWS ACCESS KEY"
  secret_access_key: "YOUR AWS SECRET KEY"
  bucket: "S3 Bucket"
{% endhighlight %}

This example is almost certainly not ideal, as you probably want to upload your built binaries and documentation. Set `skip_cleanup` to `true` to prevent Travis CI from deleting your build artifacts.

{% highlight yaml %}
deploy:
  provider: s3
  access_key_id: "YOUR AWS ACCESS KEY"
  secret_access_key: "YOUR AWS SECRET KEY"
  bucket: "S3 Bucket"
  skip_cleanup: true
{% endhighlight %}

You can find your AWS Access Keys [here](https://console.aws.amazon.com/iam/home?#security_credential). It is recommended to encrypt that key.
Assuming you have the Travis CI command line client installed, you can do it like this:

{% highlight console %}
travis encrypt --add deploy.secret_access_key
{% endhighlight %}

You will be prompted to enter your api key on the command line.

You can also have the `travis` tool set up everything for you:

{% highlight console %}
$ travis setup s3
{% endhighlight %}

Keep in mind that the above command has to run in your project directory, so it can modify the `.travis.yml` for you.

### S3 ACL via option

You can set the acl of your uploaded files via the `acl` option like this:

{% highlight yaml %}
deploy:
  provider: s3
  access_key_id: "YOUR AWS ACCESS KEY"
  secret_access_key: "YOUR AWS SECRET KEY"
  bucket: "S3 Bucket"
  skip_cleanup: true
  acl: public_read
{% endhighlight %}

Valid ACL values are: `private`, `public_read`, `public_read_write`, `authenticated_read`, `bucket_owner_read`, `bucket_owner_full_control`. The ACL defaults to `private`.

### S3 ACL with bucket policy

Another way to set ACL for your artifacts is via a S3 bucket policy.

This bucket policy grants the public read permission in a specific S3 bucket:

{% highlight json %}
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowPublicRead",
      "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3:::YOUR BUCKET NAME/*"]
    }
  ]
}
{% endhighlight %}


### S3 bucket regions

By default the region `us-east-1` is used when deploying to S3. If your bucket is hosted in a different region, deploying using the default region results in the following error.

    The bucket you are attempting to access must be addressed using the specified endpoint.
    Please send all future requests to this endpoint. (AWS::S3::Errors::PermanentRedirect)

This can be resolved by specifying your bucket's region using the `region` configuration. For example, this example uses the `eu-west-1` region.

{% highlight yaml %}
deploy:
  provider: s3
  access_key_id: "YOUR AWS ACCESS KEY"
  secret_access_key: "YOUR AWS SECRET KEY"
  bucket: "S3 Bucket"
  skip_cleanup: true
  region: eu-west-1
{% endhighlight %}

### S3 bucket endpoints

By default the endpoint `s3.amazonaws.com` is used when deploying to S3. This endpoint automatically routes to the North Virginia and the Pacific Northwest regions. You can set the endpoint by using the `endpoint` option. This example uses the Tokyo endpoint (s3-ap-northeast-1.amazonaws.com) to deploy to S3.

{% highlight yaml %}
deploy:
  provider: s3
  access_key_id: "YOUR AWS ACCESS KEY"
  secret_access_key: "YOUR AWS SECRET KEY"
  bucket: "S3 Bucket"
  skip_cleanup: true
  endpoint: s3-ap-northeast-1.amazonaws.com
{% endhighlight %}

### Deploy From Only One Folder

Often, you don't want to upload your entire project to S3. You can tell Travis CI to only upload a single folder to S3. You can use the `local_dir` option to do so. This example uploads the `build` directory of your project to S3:

{% highlight yaml %}
deploy:
  provider: s3
  access_key_id: "YOUR AWS ACCESS KEY"
  secret_access_key: "YOUR AWS SECRET KEY"
  bucket: "S3 Bucket"
  skip_cleanup: true
  local_dir: build
{% endhighlight %}

### Deploy to a Specific S3 Folder

Often, you want to upload only to a specific S3 Folder. You can use the `upload-dir` option to set the S3 destination folder. This example uploads to the `travis-builds` folder of your s3 bucket.

{% highlight yaml %}
deploy:
  provider: s3
  access_key_id: "YOUR AWS ACCESS KEY"
  secret_access_key: "YOUR AWS SECRET KEY"
  bucket: "S3 Bucket"
  skip_cleanup: true
  upload-dir: travis-builds
{% endhighlight %}

### Deploy to a S3 hosted website:

Deploying to a amazon S3 website can be a little tricky. First, you need to set a special endpoint based on your region:

| Region                               | Endpoint                                            |
|--------------------------------------|-----------------------------------------------------|
| US Standard	                       | bucket-name.s3-website-us-east-1.amazonaws.com      |
| US West (Oregon) Region              | bucket-name.s3-website-us-west-2.amazonaws.com      |
| US West (Northern California) Region | bucket-name.s3-website-us-west-1.amazonaws.com      |
| EU (Ireland) Region 	               | bucket-name.s3-website-eu-west-1.amazonaws.com      |
| Asia Pacific (Singapore) Region	   | bucket-name.s3-website-ap-southeast-1.amazonaws.com |
| Asia Pacific (Sydney) Region	       | bucket-name.s3-website-ap-southeast-2.amazonaws.com |
| Asia Pacific (Tokyo) Region	       | bucket-name.s3-website-ap-northeast-1.amazonaws.com |
| South America (Sao Paulo) Region	   | bucket-name.s3-website-sa-east-1.amazonaws.com      |

After looking up your endpoint, you should be able to use this template to upload to your website.

{% highlight yaml %}
deploy:
  provider: s3
  access_key_id: "YOUR AWS ACCESS KEY"
  secret_access_key: "YOUR AWS SECRET KEY"
  bucket: "S3 Bucket"
  skip_cleanup: true
  endpoint: "Bucket endpoint (found above)"
  region: "Bucket region"
{% endhighlight %}

Remember that you need to set the bucket to have an ACL of `public` for anybody to be able to see your website.

### Deploy to Multiple Buckets:

If you want to upload to multiple buckets, you can do this:

{% highlight yaml %}
deploy:
  - provider: s3
    access_key_id: "YOUR AWS ACCESS KEY"
    secret_access_key: "YOUR AWS SECRET KEY"
    bucket: "S3 Bucket"
    skip_cleanup: true
 - provider: s3
   access_key_id: "YOUR AWS ACCESS KEY"
   secret_access_key: "YOUR AWS SECRET KEY"
   bucket: "Second S3 Bucket"
   skip_cleanup: true
{% endhighlight %}

### Conditional releases

You can deploy only when certain conditions are met.
See [Conditional Releases with `on:`](/user/deployment#Conditional-Releases-with-on%3A).

### Running commands before and after release

Sometimes you want to run commands before or after releasing a gem. You can use the `before_deploy` and `after_deploy` stages for this. These will only be triggered if Travis CI is actually pushing a release.

{% highlight yaml %}
    before_deploy: "echo 'ready?'"
    deploy:
      ..
    after_deploy:
      - ./after_deploy_1.sh
      - ./after_deploy_2.sh
{% endhighlight %}

### Setting `Content-Encoding` header

S3 uploads can optionally set HTTP header `Content-Encoding`.
This header allows files to be sent compressed while retaining file extensions and
the associated MIME types.

To enable this feature, add:

{% highlight yaml %}
deploy:
  provider: s3
  ..
  detect_encoding: true # <== default is false
{% endhighlight %}

If the file is compressed with `gzip` or `compress`, it will be uploaded with
the appropriate header.

### HTTP cache control

S3 uploads can optionally set  `Cache-Control` and `Expires` HTTP headers.

Set HTTP header `Cache-Control` to suggest that the browser cache the file. Defaults to `no-cache`. Valid options are `no-cache`, `no-store`, `max-age=<seconds>`, `s-maxage=<seconds> no-transform`, `public`, `private`.

`Expires` sets the date and time that the cached object is no longer cacheable. Defaults to not set. The date must be in the format `YYYY-MM-DD HH:MM:SS -ZONE`.

{% highlight yaml %}
deploy:
  provider: s3
  ..
  cache_control: "max-age=31536000"
  expires: "2012-12-21 00:00:00 -0000"
{% endhighlight %}
